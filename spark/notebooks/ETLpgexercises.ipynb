{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d9c79fe-654e-4260-8dcb-3c71b9407757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this notebook, you will \n",
    "- learn the concept of ETL\n",
    "- write ETL jobs for CSV files from `pgexercises` https://pgexercises.com/gettingstarted.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd5df111-08b1-4884-9c39-8e2ca4e71195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# What's ETL or ELT?\n",
    "\n",
    "ETL stands for Extract, Transform, Load. In the context of Spark, ETL refers to the process of extracting data from various sources, transforming it into a desired format or structure, and loading it into a target system, such as a data warehouse or a data lake.\n",
    "\n",
    "Here's a breakdown of each step in the ETL process:\n",
    "\n",
    "## Extract\n",
    "This step involves extracting data from multiple sources, such as databases, files (CSV, JSON, Parquet), APIs, or streaming data sources. Spark provides connectors and APIs to read data from a wide range of sources, allowing you to extract data in parallel and efficiently handle large datasets.\n",
    "\n",
    "## Transform\n",
    "In the transform step, the extracted data is processed and transformed according to specific business logic or requirements. This may involve cleaning the data, applying calculations or aggregations, performing data enrichment, filtering, joining datasets, or any other data manipulation operations. Spark provides a powerful set of transformation functions and SQL capabilities to perform these operations efficiently in a distributed and scalable manner.\n",
    "\n",
    "## Load\n",
    "Once the data has been transformed, it is loaded into a target system, such as a data warehouse, a data lake, or another storage system. Spark allows you to write the transformed data to various output formats and storage systems, including databases, distributed file systems (like Hadoop Distributed File System or Amazon S3), or columnar formats like Delta Lake or Apache Parquet. The data can be partitioned, sorted, or structured to optimize querying and analysis.\n",
    "\n",
    "Spark's distributed computing capabilities, scalability, and rich ecosystem of libraries make it a popular choice for ETL workflows. It can handle large-scale data processing, perform complex transformations, and efficiently load data into different target systems.\n",
    "\n",
    "By leveraging Spark for ETL, organizations can extract data from diverse sources, apply transformations to ensure data quality and consistency, and load the transformed data into a central repository for further analysis, reporting, or machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48ff3413-eea1-4cfb-a0c1-c4ed0e485121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import `pgexercises` CSV files\n",
    "\n",
    "- The pgexercises CSV data files can be found [here](https://github.com/jarviscanada/jarvis_data_eng_demo/tree/feature/data/spark/data/pgexercises).\n",
    "- The pgexercises schema can be found [here](https://pgexercises.com/gettingstarted.html) (for reference purposes).\n",
    "- Upload the `bookings.csv`, `facilities.csv`, and `members.csv` files using Databricks UI (see screenshot)\n",
    "- You can view the imported files from the DBFS UI.\n",
    "\n",
    "![Upload Files](https://raw.githubusercontent.com/jarviscanada/jarvis_data_eng_demo/feature/data/spark/notebook/spark_fundamentals/img/upload%20file.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e85a8b2-6cf3-40af-92d6-74220a9ce9c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Interview Questions\n",
    "\n",
    "While completing the rest of the practice, try to answer the following questions:\n",
    "\n",
    "## Concepts\n",
    "- What is ETL? (Hint: Explain each step)\n",
    "\n",
    "## Databricks\n",
    "- What is Databricks?\n",
    "- What is a Notebook?\n",
    "- What is DBFS?\n",
    "- What is a cluster? \n",
    "- Is Databricks a data lake or a data warehouse?\n",
    "\n",
    "## Managed Table\n",
    "- What is a managed table in Databricks?\n",
    "- Can you explain how to create a managed table in Databricks?\n",
    "- Can you compare a managed table with an RDBMS table? (Hint: Schema on read vs schema on write)\n",
    "- What is the Hive metastore and how does it relate to managed tables in Databricks?\n",
    "- How does a managed table differ from an unmanaged (external) table in Databricks? (Hint: Consider what happens to the data when the table is deleted)\n",
    "- How can you define a schema for a managed table?\n",
    "\n",
    "## Spark\n",
    "`df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_location)`\n",
    "- What does the option(\"inferSchema\", \"true\") do? \n",
    "- What does the option(\"header\", \"true\") do?\n",
    "- How can you write data to a managed table?\n",
    "- How can you read data from a managed table into a DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30083149-8fbc-4a35-a134-dc14ea381a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ETL `bookings.csv` file\n",
    "\n",
    "- **Extract**: Load data from CSV file into a DF\n",
    "- **Transformation**: no transformation needed as we want to load data as it\n",
    "- **Load**: Save the DF into a managed table (or Hive table); \n",
    "\n",
    "# Managed Table\n",
    "This is an important interview topic. Some people may refer to managed tables as Hive tables.\n",
    "\n",
    "https://docs.databricks.com/data-governance/unity-catalog/create-tables.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0762a11d-a040-4b67-ab15-374eef15ed38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- bookid: integer (nullable = true)\n |-- facid: integer (nullable = true)\n |-- memid: integer (nullable = true)\n |-- starttime: timestamp (nullable = true)\n |-- slots: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\n",
    "\n",
    "file_location = \"/FileStore/tables/bookings.csv\"\n",
    "\n",
    "# What does `option(\"header\", \"true\")` and `option(\"inferSchema\", \"true\")` do?\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_location)\n",
    "\n",
    "# Why the df schema doesn't match the DDL data type? https://pgexercises.com/gettingstarted.html (hint: `option(\"inferSchema\", \"true\")`)\n",
    "df.printSchema()\n",
    "\n",
    "# Here is the solution to define schema manually\n",
    "# Define schema for the bookings table\n",
    "schema = StructType([\n",
    "    StructField(\"bookid\", IntegerType(), nullable = False),\n",
    "    StructField(\"facid\", IntegerType(), nullable = False),\n",
    "    StructField(\"memid\", IntegerType(), nullable = False),\n",
    "    StructField(\"starttime\", TimestampType(), nullable = False),\n",
    "    StructField(\"slots\", IntegerType(), nullable = False)\n",
    "])\n",
    "\n",
    "# Read data from CSV file into DataFrame with predefined schema\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(file_location)\n",
    "\n",
    "# No \n",
    "\n",
    "# Drop the table if it already exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS bookings\")\n",
    "\n",
    "# Write data from DataFrame into managed table\n",
    "df.write.saveAsTable(\"bookings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d925a21-98b7-48e7-8dd8-9d8863826747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Complete ETL Jobs\n",
    "\n",
    "- Complete ETL for `facilities.csv` and `members.csv`\n",
    "- Tips\n",
    "  - The Databricks community version will terminate the cluster after a few hours of inactivity. As a result, all managed tables will be deleted. You will need to rerun this notebook to perform the ETL on all files for the other exercises.\n",
    "  - DBFS data will not be deleted when a custer become inactive/deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdad4ceb-f2f3-4fa9-8be1-260eb2f254ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write a ETL job for `facilities.csv`\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DecimalType, IntegerType\n",
    "\n",
    "file_location_fac = '/FileStore/tables/facilities.csv'\n",
    "\n",
    "schema_fac = StructType([\n",
    "    StructField(\"facid\", IntegerType(), nullable = False),\n",
    "    StructField(\"name\", StringType(), nullable = False),\n",
    "    StructField(\"membercost\", DecimalType(), nullable = False),\n",
    "    StructField(\"guestcost\", DecimalType(), nullable = False),\n",
    "    StructField(\"initialoutlay\", DecimalType(), nullable = False),\n",
    "    StructField(\"monthlymaintenance\", DecimalType(), nullable = False)\n",
    "])\n",
    "\n",
    "df_fac = spark.read.format(\n",
    "    \"csv\"\n",
    ").option(\n",
    "    \"header\",\n",
    "    \"true\"\n",
    ").schema(schema_fac).load(file_location_fac)\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS facilities\")\n",
    "\n",
    "df_fac.write.saveAsTable(\"facilities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d372e005-1033-41b0-8742-5995a409d13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write a ETL job Complete ETL for `members.csv`\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DecimalType, IntegerType\n",
    "\n",
    "file_location_mem = '/FileStore/tables/members.csv'\n",
    "\n",
    "schema_mem = StructType([\n",
    "    StructField(\"memid\", IntegerType(), nullable = False),\n",
    "    StructField(\"surname\", StringType(), nullable = False),\n",
    "    StructField(\"firstname\", StringType(), nullable = False),\n",
    "    StructField(\"address\", StringType(), nullable = False),\n",
    "    StructField(\"zipcode\", IntegerType(), nullable = False),\n",
    "    StructField(\"telephone\", StringType(), nullable = False),\n",
    "    StructField(\"recommendedby\", IntegerType(), nullable = True),\n",
    "    StructField(\"joindate\", TimestampType(), nullable = False)\n",
    "])\n",
    "\n",
    "df_mem = spark.read.format(\n",
    "    \"csv\"\n",
    ").option(\n",
    "    \"header\",\n",
    "    \"true\"\n",
    ").schema(schema_mem).load(file_location_mem)\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS members\")\n",
    "\n",
    "df_mem.write.saveAsTable(\"members\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81731b22-2828-47ab-9e40-cdb3c194c022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---------+--------------------+-------+--------------+-------------+-------------------+\n|memid| surname|firstname|             address|zipcode|     telephone|recommendedby|           joindate|\n+-----+--------+---------+--------------------+-------+--------------+-------------+-------------------+\n|    0|   GUEST|    GUEST|               GUEST|      0|(000) 000-0000|         NULL|2012-07-01 00:00:00|\n|    1|   Smith|   Darren|8 Bloomsbury Clos...|   4321|  555-555-5555|         NULL|2012-07-02 12:02:05|\n|    2|   Smith|    Tracy|8 Bloomsbury Clos...|   4321|  555-555-5555|         NULL|2012-07-02 12:08:23|\n|    3|  Rownam|      Tim|23 Highway Way, B...|  23423|(844) 693-0723|         NULL|2012-07-03 09:32:15|\n|    4|Joplette|   Janice|20 Crossing Road,...|    234|(833) 942-4710|            1|2012-07-03 10:25:05|\n+-----+--------+---------+--------------------+-------+--------------+-------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Verify Table Content\").getOrCreate()\n",
    "query = f\"SELECT * FROM members LIMIT 5\"\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a9aa5dc-ea21-4912-963f-67678b129ef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Save your work to Git\n",
    "\n",
    "- Export the notebook to IPYTHON format, `notebook top menu bar -> File -> Export -> iphython`\n",
    "- Upload to your Git repository, `your_repo/spark/notebooks/`\n",
    "- Github can render ipython notebook https://github.com/josephcslater/JupyterExamples/blob/master/Calc_Review.ipynb"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3267011519595935,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ETLpgexercises",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
