{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be60341-fe75-47c1-b33c-d7bacb1fe4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this notebook, you will craft sophisticated ETL jobs that interface with a variety of common data sources, such as \n",
    "- REST APIs (HTTP endpoints)\n",
    "- RDBMS\n",
    "- Hive tables (managed tables)\n",
    "- Various file formats (csv, json, parquet, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9fe8dc-6b2e-4499-8961-7e01309d05f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Interview Questions\n",
    "\n",
    "As you progress through the practice, attempt to answer the following questions:\n",
    "\n",
    "## Columnar File\n",
    "- What is a columnar file format and what advantages does it offer?\n",
    "- Why is Parquet frequently used with Spark and how does it function?\n",
    "- How do you read/write data from/to a Parquet file using a DataFrame?\n",
    "\n",
    "## Partitions\n",
    "- How do you save data to a file system by partitions? (Hint: Provide the code)\n",
    "- How and why can partitions reduce query execution time? (Hint: Give an example)\n",
    "\n",
    "## JDBC and RDBMS\n",
    "- How do you load data from an RDBMS into Spark? (Hint: Discuss the steps and JDBC)\n",
    "\n",
    "## REST API and HTTP Requests\n",
    "- How can Spark be used to fetch data from a REST API? (Hint: Discuss making API requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c7f0dcb-2214-41ae-a6f4-12d5a34506ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Job One: Parquet file\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Data transformation requirements https://pgexercises.com/questions/aggregates/fachoursbymonth.html\n",
    "\n",
    "### Load\n",
    "Load data into a parquet file\n",
    "\n",
    "### What is Parquet? \n",
    "\n",
    "Columnar files are an important technique for optimizing Spark queries. Additionally, they are often tested in interviews.\n",
    "- https://www.youtube.com/watch?v=KLFadWdomyI\n",
    "- https://www.databricks.com/glossary/what-is-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33324d02-bc67-4c31-b822-8fe8c69fead5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "df_mem = spark.table('members')\n",
    "df_book = spark.table('bookings')\n",
    "df_fac = spark.table('facilities')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58db2c0f-9c2a-4f78-910e-3f3d383919b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df_etl_1 = df_book.filter(\n",
    "    (col('starttime') >= \"2012-09-01\") & (col('starttime') < \"2012-10-01\")\n",
    ").groupBy(\n",
    "    'facid'\n",
    "    ).agg(\n",
    "        sum('slots').alias(\"Total Slots\")\n",
    "        ).orderBy('Total Slots')\n",
    "\n",
    "df_etl_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cddb1973-c049-498d-81b7-11a5cfc312b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_out = \"/FileStore/data/ETLjob1\"\n",
    "\n",
    "df_etl_1.write.parquet(path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b51d425e-d532-47e5-8cbf-a91ca78246b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Job Two: Partitions\n",
    "\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Transform the data https://pgexercises.com/questions/joins/threejoin.html\n",
    "\n",
    "### Load\n",
    "Partition the result data by facility column and then save to `threejoin_delta` managed table. Additionally, they are often tested in interviews.\n",
    "\n",
    "hint: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html\n",
    "\n",
    "What are paritions? \n",
    "\n",
    "Partitions are an important technique to optimize Spark queries\n",
    "- https://www.youtube.com/watch?v=hvF7tY2-L3U&t=268s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aea2ca-5178-4034-91ee-c09942c5f518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "df_mem = spark.table('members')\n",
    "df_book = spark.table('bookings')\n",
    "df_fac = spark.table('facilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11357684-03fd-42e7-bf47-18b31931b453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "df_etl_2 = df_mem.join(\n",
    "    df_book, df_mem.memid == df_book.memid, 'inner'\n",
    ").join(\n",
    "    df_fac, df_book.facid == df_fac.facid, 'inner'\n",
    ").select(\n",
    "    concat(\n",
    "        df_mem.firstname, lit(' '), df_mem.surname\n",
    "    ).alias('member'),\n",
    "    df_fac.name.alias('facility')\n",
    ").filter(\n",
    "    col('facility').contains('Tennis Court')\n",
    ").distinct().orderBy('member', 'facility')\n",
    "\n",
    "df_etl_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b92f86e-cbc7-4607-a014-7e886745f74a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_etl_2.write.partitionBy('facility').saveAsTable('threejoin_delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f06dd2c7-09ee-42ba-aa98-c067b100e396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM threejoin_delta LIMIT 10\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7610de14-acd6-4374-945d-661dbc08a08e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Job Three: HTTP Requests\n",
    "\n",
    "### Extract\n",
    "Extract daily stock price data price from the following companies, Google, Apple, Microsoft, and Tesla. \n",
    "\n",
    "Data Source\n",
    "- API: https://rapidapi.com/alphavantage/api/alpha-vantage\n",
    "- Endpoint: GET `TIME_SERIES_DAILY`\n",
    "\n",
    "Sample HTTP request\n",
    "\n",
    "```\n",
    "curl --request GET \\\n",
    "\t--url 'https://alpha-vantage.p.rapidapi.com/query?function=TIME_SERIES_DAILY&symbol=TSLA&outputsize=compact&datatype=json' \\\n",
    "\t--header 'X-RapidAPI-Host: alpha-vantage.p.rapidapi.com' \\\n",
    "\t--header 'X-RapidAPI-Key: [YOUR_KEY]'\n",
    "\n",
    "```\n",
    "\n",
    "Sample Python HTTP request\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "querystring = {\n",
    "    \"function\":\"TIME_SERIES_DAILY\",\n",
    "    \"symbol\":\"IBM\",\n",
    "    \"datatype\":\"json\",\n",
    "    \"outputsize\":\"compact\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"[YOUR_KEY]\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Now 'data' contains the daily time series data for \"IBM\"\n",
    "```\n",
    "\n",
    "### Transform\n",
    "Find **weekly** max closing price for each company.\n",
    "\n",
    "hints: \n",
    "  - Use a `for-loop` to get stock data for each company\n",
    "  - Use the spark `union` operation to concat all data into one DF\n",
    "  - create a new `week` column from the data column\n",
    "  - use `group by` to calculate max closing price\n",
    "\n",
    "### Load\n",
    "- Partition `DF` by company\n",
    "- Load the DF in to a managed table called, `max_closing_price_weekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b76fcc5-fc12-4401-a16c-e24c4c890dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "import requests\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def get_time_series_daily(symbol):\n",
    "\n",
    "  url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "  querystring = {\n",
    "    \"function\":\"TIME_SERIES_DAILY\",\n",
    "    \"symbol\":\"MSFT\",\n",
    "    \"datatype\":\"json\"\n",
    "  }\n",
    "\n",
    "  headers = {\n",
    "\t  \"x-rapidapi-key\": \"760895d090msh1fbe4a93d8b1b98p11ae36jsnc52dd60380a7\",\n",
    "\t  \"x-rapidapi-host\": \"alpha-vantage.p.rapidapi.com\"\n",
    "  }\n",
    "\n",
    "  response = requests.get(url, headers=headers, params=querystring)\n",
    "  response = response.json()\n",
    "\n",
    "  return response['Time Series (Daily)']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8edeafbc-1ca8-44ca-940f-03d7d88702b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+---------+----------+-------+\n| 1. open| 2. high|  3. low|4. close|5. volume|      date|company|\n+--------+--------+--------+--------+---------+----------+-------+\n|392.3200|394.8000|385.5400|393.3100| 22034087|2025-03-07|   GOOG|\n|392.3200|394.8000|385.5400|393.3100| 22034087|2025-03-07|   AAPL|\n|392.3200|394.8000|385.5400|393.3100| 22034087|2025-03-07|   TSLA|\n|392.3200|394.8000|385.5400|393.3100| 22034087|2025-03-07|   MSFT|\n|392.3200|394.8000|385.5400|393.3100| 22034087|2025-03-07|   GOOG|\n+--------+--------+--------+--------+---------+----------+-------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "company_symbols = {\n",
    "    \"GOOG\",\n",
    "    \"AAPL\",\n",
    "    \"MSFT\",\n",
    "    \"TSLA\"\n",
    "}\n",
    "\n",
    "def check_df_exists():\n",
    "    try:\n",
    "        df_all_companies.count()\n",
    "        return True\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "for symbol in company_symbols:\n",
    "    response = get_time_series_daily(symbol)\n",
    "    data = [\n",
    "        {\"date\": date, **prices} for date, prices in response.items()\n",
    "    ]\n",
    "    if check_df_exists(): \n",
    "        df_new_data = spark.createDataFrame(data)\n",
    "        df_new_data = df_new_data.withColumn(\"company\", lit(symbol))\n",
    "        df_all_companies = df_all_companies.union(df_new_data)\n",
    "    else: \n",
    "        df_all_companies = spark.createDataFrame(data)\n",
    "        df_all_companies = df_all_companies.withColumn(\"company\", lit(symbol))\n",
    "    \n",
    "\n",
    "df_all_companies.filter(\n",
    "    col(\"date\") == \"2025-03-07\"\n",
    ").show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857daf8d-f9ce-435b-9ada-e9aa1ccd67ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+----------+-------+\n|    open|    high|     low|   close|  volume|      date|company|\n+--------+--------+--------+--------+--------+----------+-------+\n|392.3200|394.8000|385.5400|393.3100|22034087|2025-03-07|   GOOG|\n+--------+--------+--------+--------+--------+----------+-------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "#Rename the cols\n",
    "new_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "for old_col, new_col in zip(df_all_companies.columns[:5], new_cols):\n",
    "    df_all_companies = df_all_companies.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "df_all_companies.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb736065-be28-4c1b-981b-dea7e24ac3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+----------+-------+----+\n|    open|    high|     low|   close|  volume|      date|company|week|\n+--------+--------+--------+--------+--------+----------+-------+----+\n|392.3200|394.8000|385.5400|393.3100|22034087|2025-03-07|   GOOG|  10|\n|394.2800|402.1500|392.6777|396.8900|23304625|2025-03-06|   GOOG|  10|\n|389.3400|401.6700|388.8100|401.0200|23433132|2025-03-05|   GOOG|  10|\n+--------+--------+--------+--------+--------+----------+-------+----+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Add week column\n",
    "from pyspark.sql.functions import weekofyear, max\n",
    "\n",
    "df_all_companies_week = df_all_companies.withColumn(\n",
    "    \"week\",\n",
    "    weekofyear(\n",
    "        col('date')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_all_companies_week.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70da4c49-a733-4025-92a9-53b3100c96e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------------+\n|company|week|max_close_of_week|\n+-------+----+-----------------+\n|   AAPL|   1|         424.8300|\n|   AAPL|   2|         427.8500|\n|   AAPL|   3|         429.0300|\n|   AAPL|   4|         446.7100|\n|   AAPL|   5|         447.2000|\n|   AAPL|   6|         415.8200|\n|   AAPL|   7|         412.2200|\n|   AAPL|   8|         416.1300|\n|   AAPL|   9|         404.0000|\n|   AAPL|  10|         401.0200|\n+-------+----+-----------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Perform transformation\n",
    "df_max_close = df_all_companies_week.groupBy('company', 'week').agg(\n",
    "    max(\n",
    "        col('close')\n",
    "    ).alias(\"max_close_of_week\")\n",
    ")\n",
    "\n",
    "df_max_close.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a1e900a-2c2d-4d2c-b805-fd6196fa6b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Partition by company and load into 'max_closing_price_weekly'\n",
    "df_max_close.write.partitionBy('company').saveAsTable('max_closing_price_weekly')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb731053-545e-42a4-aad7-7fd355a1b254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------------+\n|company|week|max_close_of_week|\n+-------+----+-----------------+\n|   AAPL|   1|         424.8300|\n|   AAPL|   2|         427.8500|\n|   AAPL|   3|         429.0300|\n|   AAPL|   4|         446.7100|\n|   AAPL|   5|         447.2000|\n|   AAPL|   6|         415.8200|\n|   AAPL|   7|         412.2200|\n|   AAPL|   8|         416.1300|\n|   AAPL|   9|         404.0000|\n|   AAPL|  10|         401.0200|\n+-------+----+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM max_closing_price_weekly LIMIT 10\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f98592-1f5f-4b42-9350-6720e69a7c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Job Four: RDBMS\n",
    "\n",
    "\n",
    "### Extract\n",
    "Extract RNA data from a public PostgreSQL database.\n",
    "\n",
    "- https://rnacentral.org/help/public-database\n",
    "- Extract 100 RNA records from the `rna` table (hint: use `limit` in your sql)\n",
    "- hint: use `spark.read.jdbc` https://docs.databricks.com/external-data/jdbc.html\n",
    "\n",
    "### Transform\n",
    "We want to load the data as it so there is no transformation required.\n",
    "\n",
    "\n",
    "### Load\n",
    "Load the DF in to a managed table called, `rna_100_records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3011d775-d108-4cb0-85d1-bf21ae1c23d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "jdbc_host = \"hh-pgsql-public.ebi.ac.uk\"\n",
    "jdbc_port = \"5432\"\n",
    "jdbc_database = \"pfmegrnargs\"\n",
    "jdbc_table = \"(SELECT * FROM rna LIMIT 100) as temp_table\"\n",
    "jdbc_user = \"reader\"\n",
    "jdbc_password = \"NWDMCE5xdipIjRrp\"\n",
    "\n",
    "df_rna_table = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\n",
    "    .option(\"url\", f\"jdbc:postgresql://{jdbc_host}:{jdbc_port}/{jdbc_database}\")\n",
    "    .option(\"dbtable\", jdbc_table)  \n",
    "    .option(\"user\", jdbc_user)\n",
    "    .option(\"password\", jdbc_password)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c875ce1b-6840-4fd8-a0a3-6c368033b83e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_rna_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15c8b44c-b49f-4be9-b5c8-3912c21cfa4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_rna_table.write.saveAsTable(\"rna_100_records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd958a62-8f57-492f-806c-5e5586c5b580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM rna_100_records LIMIT 10\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sparkETL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
